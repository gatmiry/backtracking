"""
Finetune Qwen2ForClassifier on rollouts from inference_outputs.jsonl

This script loads rollouts from a jsonl file (generated by inference scripts)
and finetunes a classifier model on them.
"""

import os
import json
import torch
import transformers
import tyro
from dataclasses import dataclass
from typing import List
import ujson as json
from tqdm import tqdm

import deepseek_utils
import classifier_lib
import training_utils
import eval_helpers


@dataclass
class Args:
    # Data arguments
    rollout_file: str = "outputs/inference_outputs.jsonl"
    model_path: str = "VGS-AI/DeepSeek-VM-1.5B"  # Base classifier model to finetune
    output_dir: str = "outputs/finetuned_classifier"
    
    # Training arguments
    max_length: int = 8192
    batch_size: int = 4
    micro_batch_size: int = 4
    num_epochs: int = 3
    learning_rate: float = 1e-5
    weight_decay: float = 0.1
    grad_norm_clip: float = 1.0
    warmup_steps: int = 100
    
    # Model arguments
    attention_impl: str = "sdpa"
    gradient_checkpointing: bool = True
    p_dropout: float = 0.05
    
    # Other
    seed: int = 1337
    gpu_id: int = 0
    save_every: int = 500
    eval_every: int = 200
    num_labels: int = 1  # Binary classification (reward 0 or 1)


def load_rollouts_from_jsonl(file_path: str) -> List[dict]:
    """Load rollouts from a jsonl file."""
    data = []
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Rollout file not found: {file_path}")
    
    print(f"Loading rollouts from {file_path}...")
    with open(file_path, "r") as f:
        for line in tqdm(f, desc="Loading rollouts"):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                data.append(record)
            except json.JSONDecodeError as e:
                print(f"Warning: Skipping invalid JSON line: {e}")
                continue
    
    print(f"Loaded {len(data)} rollouts")
    return data


def prepare_dataset(rollouts: List[dict], tokenizer: transformers.PreTrainedTokenizer) -> List[dict]:
    """
    Convert rollouts to the format expected by the training code.
    
    Each sample should have:
    - roll_in_ids: tokenized problem
    - roll_outs_ids: list of generated token IDs (single element for our case)
    - labels: list of rewards (single element for our case)
    """
    dataset = []
    
    print("Preparing dataset...")
    for rollout in tqdm(rollouts, desc="Processing rollouts"):
        problem = rollout["problem"]
        generated_ids = rollout["generated_ids"]
        reward = rollout["reward"]
        
        # Format the problem using the same format as during inference
        formatted_problem = deepseek_utils.format_roll_in(problem)
        roll_in_ids = tokenizer(formatted_problem, add_special_tokens=False)["input_ids"]
        
        # Convert reward to int (0 or 1)
        label = 1 if reward else 0
        
        # Create sample in the expected format
        # For single rollout per problem, we use a list with one element
        # to match the format expected by the collator
        sample = {
            "roll_in_ids": roll_in_ids,
            "roll_outs_ids": [generated_ids],  # List with single element
            "labels": label,  # Single value (will be converted to tensor by collator)
        }
        dataset.append(sample)
    
    return dataset


def create_data_loader(dataset: List[dict], tokenizer: transformers.PreTrainedTokenizer, 
                       batch_size: int, max_length: int, shuffle: bool = True):
    """Create a DataLoader from the dataset."""
    # The dataset format matches what FlattenedDataset expects
    # We need to wrap it to work with the existing infrastructure
    class SimpleDataset:
        def __init__(self, data):
            self.data = data
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx]
    
    simple_dataset = SimpleDataset(dataset)
    
    # Create collator
    pad_multiple = max_length // 16 if max_length % 16 == 0 else None
    collate_fn = training_utils.RollInOutCollator(
        tokenizer,
        roll_in_key="roll_in_ids",
        roll_out_key="roll_outs_ids",
        max_length=max_length,
        pad_multiple=pad_multiple
    )
    
    # Create DataLoader
    data_loader = torch.utils.data.DataLoader(
        simple_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn,
        pin_memory=True,
        num_workers=2
    )
    
    return data_loader


def train_step(model, batch, optimizer, device, dtype):
    """Perform a single training step."""
    model.train()
    optimizer.zero_grad()
    
    # Move batch to device
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)  # [batch_size]
    loss_mask = batch.get("roll_out_mask", None)
    if loss_mask is not None:
        loss_mask = loss_mask.to(device)
    
    # Forward pass
    with torch.autocast(device_type="cuda", dtype=dtype):
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            loss_mask=loss_mask,
        )
        loss = outputs.loss
    
    # Backward pass
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    
    return loss.item()


@torch.no_grad()
def evaluate(model, data_loader, device, dtype):
    """Evaluate the model on a dataset."""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    correct = 0
    total = 0
    
    for batch in tqdm(data_loader, desc="Evaluating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        loss_mask = batch.get("roll_out_mask", None)
        if loss_mask is not None:
            loss_mask = loss_mask.to(device)
        
        with torch.autocast(device_type="cuda", dtype=dtype):
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                loss_mask=loss_mask,
            )
            loss = outputs.loss
        
        total_loss += loss.item()
        num_batches += 1
        
        # Compute accuracy (for binary classification)
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
            if len(logits.shape) == 3:  # [batch, seq_len, num_labels]
                # Use the last token's logits where loss_mask is 1
                if loss_mask is not None:
                    # Get logits at positions where loss_mask is 1
                    mask_positions = loss_mask.bool()  # [batch, seq_len]
                    # For each sample, get the last position where mask is 1
                    batch_size = logits.shape[0]
                    preds = []
                    for i in range(batch_size):
                        mask_pos = mask_positions[i]
                        if mask_pos.any():
                            last_pos = mask_pos.nonzero()[-1].item()
                            pred = logits[i, last_pos, 0] > 0
                        else:
                            pred = logits[i, -1, 0] > 0
                        preds.append(pred)
                    preds = torch.tensor(preds, device=device)
                else:
                    preds = logits[:, -1, 0] > 0
            else:
                preds = logits > 0
            correct += (preds == labels.bool()).sum().item()
            total += labels.size(0)
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    accuracy = correct / total if total > 0 else 0.0
    
    return avg_loss, accuracy


def main(args: Args):
    # Set device
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
    device = f"cuda:0"
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    torch.set_float32_matmul_precision("high")
    torch.manual_seed(args.seed)
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path, padding_side="left")
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Load rollouts
    rollouts = load_rollouts_from_jsonl(args.rollout_file)
    
    # Prepare dataset
    dataset = prepare_dataset(rollouts, tokenizer)
    
    # Split into train/val (80/20)
    split_idx = int(len(dataset) * 0.8)
    train_dataset = dataset[:split_idx]
    val_dataset = dataset[split_idx:]
    
    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")
    
    # Create data loaders
    train_loader = create_data_loader(
        train_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=True
    )
    val_loader = create_data_loader(
        val_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=False
    )
    
    # Load model
    print("Loading model...")
    model_loading_kwargs = dict(
        attn_implementation=args.attention_impl,
        torch_dtype=dtype,
        use_cache=False,
        attention_dropout=args.p_dropout,
        num_labels=args.num_labels,
    )
    model = classifier_lib.Qwen2ForClassifier.from_pretrained(
        args.model_path, **model_loading_kwargs
    )
    
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
    
    model.to(device)
    
    # Setup optimizer
    optimizer = training_utils.configure_optimizer(
        model,
        lr=args.learning_rate,
        betas=(0.9, 0.95),
        weight_decay=args.weight_decay,
        device_type=device_type,
    )
    
    # Learning rate scheduler
    num_training_steps = len(train_loader) * args.num_epochs
    scheduler = transformers.get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=num_training_steps,
    )
    
    # Training loop
    print("Starting training...")
    global_step = 0
    best_val_loss = float('inf')
    
    for epoch in range(args.num_epochs):
        print(f"\nEpoch {epoch + 1}/{args.num_epochs}")
        model.train()
        
        epoch_loss = 0.0
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Training epoch {epoch + 1}")):
            loss = train_step(model, batch, optimizer, device, dtype)
            epoch_loss += loss
            global_step += 1
            
            scheduler.step()
            
            # Evaluation
            if global_step % args.eval_every == 0:
                print(f"\nEvaluating at step {global_step}...")
                val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
                print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    save_path = os.path.join(args.output_dir, "best_model")
                    print(f"Saving best model to {save_path}...")
                    model.save_pretrained(save_path)
                    tokenizer.save_pretrained(save_path)
            
            # Periodic save
            if global_step % args.save_every == 0:
                save_path = os.path.join(args.output_dir, f"checkpoint_step_{global_step}")
                print(f"Saving checkpoint to {save_path}...")
                model.save_pretrained(save_path)
                tokenizer.save_pretrained(save_path)
        
        avg_epoch_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}")
    
    # Final evaluation
    print("\nFinal evaluation...")
    val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
    print(f"Final Val Loss: {val_loss:.4f}, Final Val Accuracy: {val_accuracy:.4f}")
    
    # Save final model
    final_save_path = os.path.join(args.output_dir, "final_model")
    print(f"Saving final model to {final_save_path}...")
    model.save_pretrained(final_save_path)
    tokenizer.save_pretrained(final_save_path)
    
    print("Training complete!")


if __name__ == "__main__":
    args = tyro.cli(Args)
    main(args)

