"""
Finetune Qwen2ForClassifier on rollouts from inference_outputs.jsonl

This script loads rollouts from a jsonl file (generated by inference scripts)
and finetunes a classifier model on them.
"""

import os
import json
import torch
import transformers
import tyro
from dataclasses import dataclass
from typing import List
import ujson as json
from tqdm import tqdm

import deepseek_utils
import classifier_lib
import training_utils
import eval_helpers
from torch.utils.data import Subset

@dataclass
class Args:
    # Data arguments
    rollout_file: str = "outputs/inference_outputs.jsonl"
    model_path: str = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"  # Base classifier model to finetune
    output_dir: str = "outputs/finetuned_classifier"
    dataset_path: str = "VGS-AI/OpenR1-VM" #"open-r1/OpenR1-Math-220k" #"/data/datasets/train_dataset"
    # Training arguments
    max_length: int = 8192
    micro_batch_size: int = 8
    num_epochs: int = 3
    learning_rate: float = 1e-5
    weight_decay: float = 0.1
    grad_norm_clip: float = 1.0
    warmup_steps: int = 100
    ddp_rank: int = 0
    ddp_world_size: int = 1
    subset_size: int = -1

    # Model arguments
    attention_impl: str = "sdpa"
    gradient_checkpointing: bool = True
    p_dropout: float = 0.05
    
    # Other
    seed: int = 1337
    gpu_id: int = 0
    save_every: int = 500
    eval_every: int = 200
    num_labels: int = 3  # Binary classification (reward 0 or 1)
    data_num_response: int = 56
    small_group_size: int = 2

def load_rollouts_from_jsonl(file_path: str) -> List[dict]:
    """Load rollouts from a jsonl file."""
    data = []
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Rollout file not found: {file_path}")
    
    print(f"Loading rollouts from {file_path}...")
    with open(file_path, "r") as f:
        for line in tqdm(f, desc="Loading rollouts"):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                data.append(record)
            except json.JSONDecodeError as e:
                print(f"Warning: Skipping invalid JSON line: {e}")
                continue
    
    print(f"Loaded {len(data)} rollouts")
    return data

import datasets
def prepare_dataset(train_dataset: datasets.Dataset, tokenizer: transformers.PreTrainedTokenizer, data_num_response: int = 56, small_group_size: int = 2) -> training_utils.FlattenedDataset:
    """
    Convert train_dataset in the format in  data_path: str = "VGS-AI/OpenR1-VM"

    data_num_response or big group size is the number of rollouts for each roll_in_ids
    small group size is just a subgrouping of the big group size of the problems. this variable affect the group_boundaries in the flattened dataset.
    """
    
    # Check if dataset has required keys
    required_keys = ['roll_outs_ids', 'labels', 'roll_in_ids']
    if len(train_dataset) > 0:
        sample = train_dataset[0]
        available_keys = list(sample.keys())
        missing_keys = [k for k in required_keys if k not in available_keys]
        
        if missing_keys:
            raise KeyError(
                f"Dataset is missing required keys: {missing_keys}. "
                f"Available keys: {available_keys}. "
                f"The dataset should be in the format from 'VGS-AI/OpenR1-VM' with keys: {required_keys}. "
                f"Please use a preprocessed dataset or add a transformation step to create these keys."
            )

    #print(f'Im at the flattenedDataset instructor')
    flattened_dataset = training_utils.FlattenedDataset(train_dataset, grouped_keys=['roll_outs_ids', 'labels', 'roll_in_ids'], shared_keys=[], big_group_size=data_num_response, small_group_size=small_group_size)
    return flattened_dataset

import training_utils
def create_data_loader(flattened_dataset: torch.utils.data.Dataset, tokenizer: transformers.PreTrainedTokenizer, 
                       batch_size: int, max_length: int, shuffle: bool = True, ddp_rank: int = 0, ddp_world_size: int = 1):
    """Create a DataLoader from the dataset."""
    # The dataset format matches what FlattenedDataset expects
    
    # Create collator
    pad_multiple = max_length // 16 if max_length % 16 == 0 else None
    collate_fn = training_utils.RollInOutCollator(
        tokenizer,
        roll_in_key="roll_in_ids",
        roll_out_key="roll_outs_ids",
        max_length=max_length,
        pad_multiple=pad_multiple
    )
    collate_fn = training_utils.RollInOutCollator(tokenizer, roll_in_key='roll_in_ids', roll_out_key='roll_outs_ids', max_length=max_length, pad_multiple=pad_multiple)
    train_sampler = training_utils.EndlessSampler(flattened_dataset, batch_size=batch_size, process_rank=ddp_rank, num_processes=ddp_world_size, shuffle=shuffle)
    data_loader = torch.utils.data.DataLoader(flattened_dataset, batch_sampler=train_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=8)
    
    return data_loader


def train_step(model, batch, optimizer, device, dtype):
    """Perform a single training step."""
    model.train()
    optimizer.zero_grad()
    
    # Move batch to device
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)  # [batch_size]
    loss_mask = batch.get("roll_out_mask", None)
    if loss_mask is not None:
        loss_mask = loss_mask.to(device)
    
    # Forward pass
    with torch.autocast(device_type="cuda", dtype=dtype):
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            loss_mask=loss_mask,
        )
        loss = outputs.loss
    
    # Backward pass
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    
    return loss.item()


@torch.no_grad()
def evaluate(model, data_loader, device, dtype):
    """Evaluate the model on a dataset."""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    correct = 0
    total = 0
    
    for batch in tqdm(data_loader, desc="Evaluating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        loss_mask = batch.get("roll_out_mask", None)
        if loss_mask is not None:
            loss_mask = loss_mask.to(device)
        
        with torch.autocast(device_type="cuda", dtype=dtype):
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                loss_mask=loss_mask,
            )
            loss = outputs.loss
        
        total_loss += loss.item()
        num_batches += 1
        
        # Compute accuracy (for binary classification)
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
            if len(logits.shape) == 3:  # [batch, seq_len, num_labels]
                # Use the last token's logits where loss_mask is 1
                if loss_mask is not None:
                    # Get logits at positions where loss_mask is 1
                    mask_positions = loss_mask.bool()  # [batch, seq_len]
                    # For each sample, get the last position where mask is 1
                    batch_size = logits.shape[0]
                    preds = []
                    for i in range(batch_size):
                        mask_pos = mask_positions[i]
                        if mask_pos.any():
                            last_pos = mask_pos.nonzero()[-1].item()
                            pred = logits[i, last_pos, 0] > 0
                        else:
                            pred = logits[i, -1, 0] > 0
                        preds.append(pred)
                    preds = torch.tensor(preds, device=device)
                else:
                    preds = logits[:, -1, 0] > 0
            else:
                preds = logits > 0
            correct += (preds == labels.bool()).sum().item()
            total += labels.size(0)
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    accuracy = correct / total if total > 0 else 0.0
    
    return avg_loss, accuracy


def main(args: Args):
    # Set device
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
    device = f"cuda:0"
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    #torch.set_float32_matmul_precision("high")
    torch.manual_seed(args.seed)
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path, padding_side="left")
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Load rollouts
    #rollouts = load_rollouts_from_jsonl(args.rollout_file)
    print(f"Loading dataset from {args.dataset_path}...")
    #raw_dataset = datasets.load_from_disk(args.dataset_path)
    raw_dataset = datasets.load_dataset(args.dataset_path, split="train")
    print(f"finished loading dataset from {args.dataset_path}")

    # Prepare dataset
    subset_size = len(raw_dataset) if args.subset_size == -1 else min(args.subset_size, len(raw_dataset))
    raw_dataset = raw_dataset.select(range(subset_size))
    print(f"selected {subset_size} rows from the dataset")
    split_idx = int(len(raw_dataset) * 0.8)
    print(f"Splitting dataset into train and val...")
    train_dataset = prepare_dataset(raw_dataset.select(range(split_idx)), tokenizer, data_num_response=args.data_num_response, small_group_size=args.small_group_size)
    val_dataset = prepare_dataset(raw_dataset.select(range(split_idx, len(raw_dataset))), tokenizer, data_num_response=args.data_num_response, small_group_size=args.small_group_size)
    print(f"finished splitting dataset into train and val")

    ##this is a torch.utils.data dataset, make sure it has len attribute 
    # Split into train/val (80/20)
    #val_dataset = Subset(dataset, range(split_idx, len(dataset)))
    
    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")
    
    # Create data loaders
    print(f"Creating data loader with micro_batch_size={args.micro_batch_size}, ddp_world_size={args.ddp_world_size}")
    train_loader = create_data_loader(
        train_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=True, ddp_rank=args.ddp_rank, ddp_world_size=args.ddp_world_size
    )

    num_batches = len(train_loader)
    print(f"Number of batches: {num_batches}, Dataset length: {len(train_dataset)}, Batch size: {args.micro_batch_size}, Expected batches: {len(train_dataset) // (args.micro_batch_size * args.ddp_world_size)}")
    print(f"num training batches: {num_batches}")
    val_loader = create_data_loader(
        val_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=False, ddp_rank=args.ddp_rank, ddp_world_size=args.ddp_world_size
    )
    
    # Load model
    print("Loading model...")
    model_loading_kwargs = dict(
        attn_implementation=args.attention_impl,
        torch_dtype=dtype,
        use_cache=False,
        attention_dropout=args.p_dropout,
        num_labels=args.num_labels,
    )
    model = classifier_lib.Qwen2ForClassifier.from_pretrained(
        args.model_path, **model_loading_kwargs
    )
    
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
    
    model.to(device)
    
    # Setup optimizer
    optimizer = training_utils.configure_optimizer(
        model,
        lr=args.learning_rate,
        betas=(0.9, 0.95),
        weight_decay=args.weight_decay,
        device_type=device_type,
    )
    
    # Learning rate scheduler
    num_training_steps = len(train_loader) * args.num_epochs
    print(f"num_training_steps: {num_training_steps}")
    scheduler = transformers.get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=num_training_steps,
    )
    
    # Training loop
    print("Starting training...")
    global_step = 0
    best_val_loss = float('inf')
    
    for epoch in range(args.num_epochs):
        print(f"\nEpoch {epoch + 1}/{args.num_epochs}")
        model.train()
        
        epoch_loss = 0.0
        #batches_per_epoch = len(train_loader)
        for batch_idx, batch in enumerate(train_loader):
            #if batch_idx >= batches_per_epoch:
            #    break
            batch = {k: v.to(device) for k, v in batch.items()}#batch =batch.to(device)
            print(f"Training batch {batch_idx} model device {model.device} batch device {batch['input_ids'].device}")
            loss = train_step(model, batch, optimizer, device, dtype)
            epoch_loss += loss
            global_step += 1
            
            scheduler.step()
            
            # Evaluation
            if global_step % args.eval_every == 0:
                print(f"\nEvaluating at step {global_step}...")
                val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
                print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    save_path = os.path.join(args.output_dir, "best_model")
                    print(f"Saving best model to {save_path}...")
                    model.save_pretrained(save_path)
                    tokenizer.save_pretrained(save_path)
            
            # Periodic save
            if global_step % args.save_every == 0:
                save_path = os.path.join(args.output_dir, f"checkpoint_step_{global_step}")
                print(f"Saving checkpoint to {save_path}...")
                model.save_pretrained(save_path)
                tokenizer.save_pretrained(save_path)
        
        avg_epoch_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}")
    
    # Final evaluation
    print("\nFinal evaluation...")
    val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
    print(f"Final Val Loss: {val_loss:.4f}, Final Val Accuracy: {val_accuracy:.4f}")
    
    # Save final model
    final_save_path = os.path.join(args.output_dir, "final_model")
    print(f"Saving final model to {final_save_path}...")
    model.save_pretrained(final_save_path)
    tokenizer.save_pretrained(final_save_path)
    
    print("Training complete!")


if __name__ == "__main__":
    args = tyro.cli(Args)
    main(args)

