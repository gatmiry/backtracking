"""
Finetune Qwen2ForClassifier on rollouts from inference_outputs.jsonl

This script loads rollouts from a jsonl file (generated by inference scripts)
and finetunes a classifier model on them.

Multi-GPU version using DistributedDataParallel.
Run with: torchrun --nproc_per_node=<num_gpus> finetune_classifier_from_rollouts_multigpu.py ...
"""

import os
import json
import torch
import transformers
import tyro
from dataclasses import dataclass
from typing import List
import ujson as json
from tqdm import tqdm
from torch.distributed import init_process_group, destroy_process_group
from torch.nn.parallel import DistributedDataParallel

import deepseek_utils
import classifier_lib
import training_utils
import eval_helpers
from torch.utils.data import Subset

@dataclass
class Args:
    # Data arguments
    rollout_file: str = "outputs/inference_outputs.jsonl"
    model_path: str = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"  # Base classifier model to finetune
    output_dir: str = "outputs/finetuned_classifier"
    dataset_path: str = "VGS-AI/OpenR1-VM" #"open-r1/OpenR1-Math-220k" #"/data/datasets/train_dataset"
    # Training arguments
    max_length: int = 12288
    micro_batch_size: int = 12  # Batch size per GPU. Effective batch size = micro_batch_size * num_gpus
    num_epochs: int = 1
    learning_rate: float = 1e-5
    weight_decay: float = 0.1
    grad_norm_clip: float = 1.0
    warmup_steps: int = 100
    ddp_rank: int = 0
    ddp_world_size: int = 1
    subset_size: int = -1

    # Model arguments
    attention_impl: str = "sdpa"
    gradient_checkpointing: bool = True
    p_dropout: float = 0.05
    
    # Other
    seed: int = 1337
    gpu_id: int = 0
    save_every: int = 500
    eval_every: int = 200
    num_labels: int = 3  # Binary classification (reward 0 or 1)
    data_num_response: int = 56
    small_group_size: int = 2

def load_rollouts_from_jsonl(file_path: str) -> List[dict]:
    """Load rollouts from a jsonl file."""
    data = []
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Rollout file not found: {file_path}")
    
    print(f"Loading rollouts from {file_path}...")
    with open(file_path, "r") as f:
        for line in tqdm(f, desc="Loading rollouts"):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                data.append(record)
            except json.JSONDecodeError as e:
                print(f"Warning: Skipping invalid JSON line: {e}")
                continue
    
    print(f"Loaded {len(data)} rollouts")
    return data

import datasets
def prepare_dataset(train_dataset: datasets.Dataset, tokenizer: transformers.PreTrainedTokenizer, data_num_response: int = 56, small_group_size: int = 2) -> training_utils.FlattenedDataset:
    """
    Convert train_dataset in the format in  data_path: str = "VGS-AI/OpenR1-VM"

    data_num_response or big group size is the number of rollouts for each roll_in_ids
    small group size is just a subgrouping of the big group size of the problems. this variable affect the group_boundaries in the flattened dataset.
    """
    
    # Check if dataset has required keys
    required_keys = ['roll_outs_ids', 'labels', 'roll_in_ids']
    if len(train_dataset) > 0:
        sample = train_dataset[0]
        available_keys = list(sample.keys())
        missing_keys = [k for k in required_keys if k not in available_keys]
        
        if missing_keys:
            raise KeyError(
                f"Dataset is missing required keys: {missing_keys}. "
                f"Available keys: {available_keys}. "
                f"The dataset should be in the format from 'VGS-AI/OpenR1-VM' with keys: {required_keys}. "
                f"Please use a preprocessed dataset or add a transformation step to create these keys."
            )

    #print(f'Im at the flattenedDataset instructor')
    flattened_dataset = training_utils.FlattenedDataset(train_dataset, grouped_keys=['roll_outs_ids', 'labels', 'roll_in_ids'], shared_keys=[], big_group_size=data_num_response, small_group_size=small_group_size)
    return flattened_dataset

import training_utils
def create_data_loader(flattened_dataset: torch.utils.data.Dataset, tokenizer: transformers.PreTrainedTokenizer, 
                       batch_size: int, max_length: int, shuffle: bool = True, ddp_rank: int = 0, ddp_world_size: int = 1):
    """Create a DataLoader from the dataset."""
    # The dataset format matches what FlattenedDataset expects
    
    # Create collator
    pad_multiple = max_length // 16 if max_length % 16 == 0 else None
    collate_fn = training_utils.RollInOutCollator(
        tokenizer,
        roll_in_key="roll_in_ids",
        roll_out_key="roll_outs_ids",
        max_length=max_length,
        pad_multiple=pad_multiple
    )
    collate_fn = training_utils.RollInOutCollator(tokenizer, roll_in_key='roll_in_ids', roll_out_key='roll_outs_ids', max_length=max_length, pad_multiple=pad_multiple)
    train_sampler = training_utils.EndlessSampler(flattened_dataset, batch_size=batch_size, process_rank=ddp_rank, num_processes=ddp_world_size, shuffle=shuffle)
    data_loader = torch.utils.data.DataLoader(flattened_dataset, batch_sampler=train_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=8)
    
    return data_loader


def train_step(model, batch, optimizer, device, dtype):
    """Perform a single training step."""
    model.train()
    optimizer.zero_grad()
    
    # Move batch to device
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)  # [batch_size]
    loss_mask = batch.get("roll_out_mask", None)
    if loss_mask is not None:
        loss_mask = loss_mask.to(device)
    
    # Forward pass
    with torch.autocast(device_type="cuda", dtype=dtype):
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            loss_mask=loss_mask,
        )
        loss = outputs.loss
    
    # Backward pass
    # DDP automatically synchronizes gradients across all GPUs during backward()
    # by performing an all-reduce operation that averages gradients
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    # After backward(), all processes have the same averaged gradients
    optimizer.step()  # All processes update with the same averaged gradients
    
    return loss.item()


@torch.no_grad()
def evaluate(model, data_loader, device, dtype):
    """Evaluate the model on a dataset."""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    correct = 0
    total = 0
    
    for batch in tqdm(data_loader, desc="Evaluating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        loss_mask = batch.get("roll_out_mask", None)
        if loss_mask is not None:
            loss_mask = loss_mask.to(device)
        
        with torch.autocast(device_type="cuda", dtype=dtype):
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                loss_mask=loss_mask,
            )
            loss = outputs.loss
        
        total_loss += loss.item()
        num_batches += 1
        
        # Compute accuracy (for binary classification)
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
            if len(logits.shape) == 3:  # [batch, seq_len, num_labels]
                # Use the last token's logits where loss_mask is 1
                if loss_mask is not None:
                    # Get logits at positions where loss_mask is 1
                    mask_positions = loss_mask.bool()  # [batch, seq_len]
                    # For each sample, get the last position where mask is 1
                    batch_size = logits.shape[0]
                    preds = []
                    for i in range(batch_size):
                        mask_pos = mask_positions[i]
                        if mask_pos.any():
                            last_pos = mask_pos.nonzero()[-1].item()
                            pred = logits[i, last_pos, 0] > 0
                        else:
                            pred = logits[i, -1, 0] > 0
                        preds.append(pred)
                    preds = torch.tensor(preds, device=device)
                else:
                    preds = logits[:, -1, 0] > 0
            else:
                preds = logits > 0
            correct += (preds == labels.bool()).sum().item()
            total += labels.size(0)
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    accuracy = correct / total if total > 0 else 0.0
    
    return avg_loss, accuracy


def main(args: Args):
    # Check if DDP is enabled (torchrun sets RANK env var)
    ddp = int(os.environ.get('RANK', -1)) != -1
    ddp_rank = 0
    ddp_local_rank = 0
    ddp_world_size = 1
    
    try:
        if ddp:
            init_process_group(backend='nccl')
            ddp_rank = int(os.environ['RANK'])
            ddp_local_rank = int(os.environ['LOCAL_RANK'])
            ddp_world_size = int(os.environ['WORLD_SIZE'])
            device = f"cuda:{ddp_local_rank}"
            torch.cuda.set_device(device)
            master_process = ddp_rank == 0
            print(f"GPU {ddp_local_rank=} | RANK {ddp_rank=} | WORLD_SIZE {ddp_world_size=}", flush=True)
        else:
            ddp_rank = 0
            ddp_local_rank = 0
            ddp_world_size = 1
            master_process = True
            # Set device
            os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
            device = f"cuda:0"
        
        device_type = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
        
        #torch.set_float32_matmul_precision("high")
        torch.manual_seed(args.seed)
        
        # Create output directory (only on master process)
        if master_process:
            os.makedirs(args.output_dir, exist_ok=True)
        
        # Load tokenizer
        if master_process:
            print("Loading tokenizer...")
        tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path, padding_side="left")
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # Load rollouts
        #rollouts = load_rollouts_from_jsonl(args.rollout_file)
        if master_process:
            print(f"Loading dataset from {args.dataset_path}...")
        #raw_dataset = datasets.load_from_disk(args.dataset_path)
        raw_dataset = datasets.load_dataset(args.dataset_path, split="train")
        if master_process:
            print(f"finished loading dataset from {args.dataset_path}")

        # Prepare dataset
        subset_size = len(raw_dataset) if args.subset_size == -1 else min(args.subset_size, len(raw_dataset))
        raw_dataset = raw_dataset.select(range(subset_size))
        if master_process:
            print(f"selected {subset_size} rows from the dataset")
        split_idx = int(len(raw_dataset) * 0.8)
        if master_process:
            print(f"Splitting dataset into train and val...")
        train_dataset = prepare_dataset(raw_dataset.select(range(split_idx)), tokenizer, data_num_response=args.data_num_response, small_group_size=args.small_group_size)
        val_dataset = prepare_dataset(raw_dataset.select(range(split_idx, len(raw_dataset))), tokenizer, data_num_response=args.data_num_response, small_group_size=args.small_group_size)
        if master_process:
            print(f"finished splitting dataset into train and val")

        ##this is a torch.utils.data dataset, make sure it has len attribute 
        # Split into train/val (80/20)
        #val_dataset = Subset(dataset, range(split_idx, len(dataset)))
        
        if master_process:
            print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")
        
        # Create data loaders
        # Each GPU processes micro_batch_size samples. DDP averages gradients across all GPUs,
        # so effective batch size for gradient update = micro_batch_size * ddp_world_size
        if master_process:
            effective_batch_size = args.micro_batch_size * ddp_world_size
            print(f"Creating data loader with micro_batch_size={args.micro_batch_size} per GPU, ddp_world_size={ddp_world_size}, effective_batch_size={effective_batch_size}")
        train_loader = create_data_loader(
            train_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=True, ddp_rank=ddp_rank, ddp_world_size=ddp_world_size
        )

        num_batches = len(train_loader)
        if master_process:
            effective_batch_size = args.micro_batch_size * ddp_world_size
            print(f"Number of batches: {num_batches}, Dataset length: {len(train_dataset)}")
            print(f"Batch size per GPU: {args.micro_batch_size}, Effective batch size (all GPUs): {effective_batch_size}")
            print(f"Expected batches per epoch: {len(train_dataset) // effective_batch_size}")
            print(f"num training batches: {num_batches}")
        val_loader = create_data_loader(
            val_dataset, tokenizer, args.micro_batch_size, args.max_length, shuffle=False, ddp_rank=ddp_rank, ddp_world_size=ddp_world_size
        )
        
        # Load model
        if master_process:
            print("Loading model...")
        model_loading_kwargs = dict(
            attn_implementation=args.attention_impl,
            torch_dtype=dtype,
            use_cache=False,
            attention_dropout=args.p_dropout,
            num_labels=args.num_labels,
        )
        model = classifier_lib.Qwen2ForClassifier.from_pretrained(
            args.model_path, **model_loading_kwargs
        )
        
        if args.gradient_checkpointing:
            model.gradient_checkpointing_enable()
        
        model.to(device)
        
        # Wrap model with DDP if using distributed training
        if ddp:
            model = DistributedDataParallel(
                model,
                device_ids=[ddp_local_rank],
                find_unused_parameters=True,
                static_graph=True,
            )
        
        # Setup optimizer
        optimizer = training_utils.configure_optimizer(
            model.module if ddp else model,
            lr=args.learning_rate,
            betas=(0.9, 0.95),
            weight_decay=args.weight_decay,
            device_type=device_type,
        )
        
        # Learning rate scheduler
        num_training_steps = len(train_loader) * args.num_epochs
        if master_process:
            print(f"num_training_steps: {num_training_steps}")
        scheduler = transformers.get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=num_training_steps,
        )
        
        # Training loop
        if master_process:
            print("Starting training...")
        global_step = 0
        best_val_loss = float('inf')
        
        for epoch in range(args.num_epochs):
            if master_process:
                print(f"\nEpoch {epoch + 1}/{args.num_epochs}")
            model.train()
            
            epoch_loss = 0.0
            #batches_per_epoch = len(train_loader)
            for batch_idx, batch in enumerate(train_loader):
                #if batch_idx >= batches_per_epoch:
                #    break
                batch = {k: v.to(device) for k, v in batch.items()}#batch =batch.to(device)
                if master_process and batch_idx % 100 == 0:
                    print(f"Training batch {batch_idx}")
                loss = train_step(model, batch, optimizer, device, dtype)
                epoch_loss += loss
                global_step += 1
                
                scheduler.step()
                
                # Evaluation
                if global_step % args.eval_every == 0:
                    if ddp:
                        torch.distributed.barrier()  # Synchronize before eval
                    if master_process:
                        print(f"\nEvaluating at step {global_step}...")
                    val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
                    if master_process:
                        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")
                    
                    # Save best model (only on master process)
                    if master_process and val_loss < best_val_loss:
                        best_val_loss = val_loss
                        save_path = os.path.join(args.output_dir, "best_model")
                        print(f"Saving best model to {save_path}...")
                        model_to_save = model.module if ddp else model
                        model_to_save.save_pretrained(save_path)
                        tokenizer.save_pretrained(save_path)
                    if ddp:
                        torch.distributed.barrier()  # Wait for save to complete
                
                # Periodic save
                if global_step % args.save_every == 0:
                    if ddp:
                        torch.distributed.barrier()  # Synchronize before save
                    if master_process:
                        save_path = os.path.join(args.output_dir, f"checkpoint_step_{global_step}")
                        print(f"Saving checkpoint to {save_path}...")
                        model_to_save = model.module if ddp else model
                        model_to_save.save_pretrained(save_path)
                        tokenizer.save_pretrained(save_path)
                    if ddp:
                        torch.distributed.barrier()  # Wait for save to complete
            
            avg_epoch_loss = epoch_loss / len(train_loader)
            if master_process:
                print(f"Epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}")
        
        # Final evaluation
        if ddp:
            torch.distributed.barrier()
        if master_process:
            print("\nFinal evaluation...")
        val_loss, val_accuracy = evaluate(model, val_loader, device, dtype)
        if master_process:
            print(f"Final Val Loss: {val_loss:.4f}, Final Val Accuracy: {val_accuracy:.4f}")
        
        # Save final model (only on master process)
        if master_process:
            final_save_path = os.path.join(args.output_dir, "final_model")
            print(f"Saving final model to {final_save_path}...")
            model_to_save = model.module if ddp else model
            model_to_save.save_pretrained(final_save_path)
            tokenizer.save_pretrained(final_save_path)
        
        if master_process:
            print("Training complete!")
        
        if ddp:
            destroy_process_group()
    except Exception as e:
        import traceback
        import sys
        rank = os.environ.get('RANK', 'N/A')
        print(f"ERROR in main (rank {rank}): {e}", file=sys.stderr, flush=True)
        traceback.print_exc(file=sys.stderr)
        if ddp:
            try:
                destroy_process_group()
            except:
                pass
        sys.exit(1)


if __name__ == "__main__":
    import traceback
    import sys
    try:
        args = tyro.cli(Args)
        main(args)
    except Exception as e:
        print(f"Error in main: {e}", file=sys.stderr)
        traceback.print_exc()
        sys.exit(1)

